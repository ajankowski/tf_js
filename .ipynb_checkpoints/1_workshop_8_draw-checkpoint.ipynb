{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rozpoznawanie odręcznych rysunków\n",
    "\n",
    "Korzystając z Google Quick Draw dataset (https://github.com/googlecreativelab/quickdraw-dataset)  \n",
    "The dataset contains around 50 million drawings of 345 classes.\n",
    "\n",
    "<img src = 'quickdraw.png' ?>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "We will train the model on GPU for free on Google Colab using Keras then run it on the browser directly using TensorFlow.js(tfjs) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Flatten, Convolution2D, MaxPooling2D, Dense\n",
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "Since we have a limited memory we will not train on all the classes. We will only use 100 classes of the dataset. The data for each class is available on Google Cloud as numpy arrays of the shape [N,784] where N is the number of of the images for that particular class. We first download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data_c/classes.txt') as f:\n",
    "#     classes = f.readlines()\n",
    "# classes = [c.replace('\\n', '') for c in classes]\n",
    "# classes = [c.replace(' ', '_') for c in classes]\n",
    "# print(classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['drums', 'spider', 'flowers', 'clock', 'airplane', 'face', 'sun', 'bicycle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "def download():\n",
    "    base = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
    "    list = []\n",
    "    for c in class_names:\n",
    "        cls_url = c.replace('_', '%20')\n",
    "        path = base + cls_url + '.npy'\n",
    "        list.append(path)\n",
    "        #urllib.request.urlretrieve(path, 'data/'+ c)\n",
    "    return list\n",
    "\n",
    "lista = download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('linki.txt', 'w') as f:\n",
    "    for link in lista:\n",
    "        f.write('{}\\n'.format(link))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since our memory is limited we will only load to memory 5000 images per classes.  \n",
    "# We also reserve 20% of the data unseen for testing\n",
    "\n",
    "def load_data(root, vfold_ratio = 0.2, max_items_per_class = 5000):\n",
    "    all_files = glob.glob(os.path.join(root, '*.npy'))\n",
    "    \n",
    "    x = np.empty([0, 784])\n",
    "    y = np.empty([0])\n",
    "    class_names = []\n",
    "    class_index = {}\n",
    "    \n",
    "    for file_number, file in enumerate(all_files):\n",
    "        data = np.load(file)\n",
    "        data = data[0: max_items_per_class, : ]\n",
    "        labels = np.full(data.shape[0], file_number)\n",
    "        \n",
    "        x = np.concatenate((x, data), axis = 0)\n",
    "        y = np.append(y, labels)\n",
    "        \n",
    "        class_name, ext = os.path.splitext(os.path.basename(file))\n",
    "        \n",
    "        class_names.append(class_name)\n",
    "        class_index[file_number] = class_name\n",
    "        \n",
    "    data = None\n",
    "    labels = None\n",
    "    \n",
    "    #separate into training and testing\n",
    "    permutation = np.random.permutation(y.shape[0])\n",
    "    x = x[permutation, : ]\n",
    "    y = y[permutation]\n",
    "    \n",
    "    vfold_size = int(x.shape[0]/100*(vfold_ratio * 100))\n",
    "    \n",
    "    x_test = x[0:vfold_size, :]\n",
    "    y_test = y[0:vfold_size]\n",
    "    \n",
    "    x_train = x[vfold_size: x.shape[0], :]\n",
    "    y_train = y[vfold_size:y.shape[0]]\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test, class_names, class_index\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Data\n",
    "We preprocess the data to prepare it for training.  \n",
    "The model will take batches of the shape [N, 28, 28, 1] and outputs probabilities of the shape [N, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test, class_names, class_index = load_data('data')\n",
    "\n",
    "num_classes = len(class_names)\n",
    "image_size = 28\n",
    "\n",
    "x_train = 255.0 - x_train\n",
    "x_test = 255.0 - x_test\n",
    "\n",
    "x_train /= 255.0\n",
    "x_test /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32000, 784), (32000,))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'airplane',\n",
       " 1: 'bicycle',\n",
       " 2: 'clock',\n",
       " 3: 'drums',\n",
       " 4: 'face',\n",
       " 5: 'flower',\n",
       " 6: 'spider',\n",
       " 7: 'sun'}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sun\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "obrazek = 562\n",
    "\n",
    "plt.imshow(x_train[obrazek].reshape(28,28), cmap = 'gray')\n",
    "print(class_index[y_train[obrazek]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], image_size, image_size, 1).astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0], image_size, image_size, 1).astype('float32')\n",
    "\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a simple CNN. Notice that the simpler the model with lesser number of parameters the better. Indeed, we will run the model after conversion on the browser and we want the model to run fast for prediction. The following model contains 3 conv layers and 2 dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(16, (3,3), padding='same', activation = 'relu', input_shape = x_train.shape[1:]))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(32, (3, 2), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(64, (3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "model.add(Dense(8, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 32)        3104      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               73856     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 1032      \n",
      "=================================================================\n",
      "Total params: 96,648\n",
      "Trainable params: 96,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28800 samples, validate on 3200 samples\n",
      "Epoch 1/5\n",
      "28800/28800 [==============================] - 27s 924us/step - loss: 1.1684 - acc: 0.6081 - val_loss: 0.6794 - val_acc: 0.7738\n",
      "Epoch 2/5\n",
      "28800/28800 [==============================] - 27s 948us/step - loss: 0.5631 - acc: 0.8175 - val_loss: 0.4037 - val_acc: 0.8722\n",
      "Epoch 3/5\n",
      "28800/28800 [==============================] - 30s 1ms/step - loss: 0.4108 - acc: 0.8683 - val_loss: 0.3395 - val_acc: 0.8944\n",
      "Epoch 4/5\n",
      "28800/28800 [==============================] - 27s 934us/step - loss: 0.3488 - acc: 0.8892 - val_loss: 0.3128 - val_acc: 0.8966\n",
      "Epoch 5/5\n",
      "28800/28800 [==============================] - 27s 933us/step - loss: 0.3123 - acc: 0.9017 - val_loss: 0.2748 - val_acc: 0.9141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26782cfada0>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_train, y=y_train, validation_split=0.1, batch_size=256, epochs=5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 3s 338us/step\n",
      "0.90\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test)\n",
    "print(f'{score[1]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the model for Web Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('quickdraw.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflowjs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!tensorflowjs_converter --input_format keras quickdraw.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
