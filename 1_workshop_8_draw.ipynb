{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rozpoznawanie odręcznych rysunków\n",
    "\n",
    "Korzystając z Google Quick Draw dataset (https://github.com/googlecreativelab/quickdraw-dataset)  \n",
    "The dataset contains around 50 million drawings of 345 classes.\n",
    "\n",
    "<img src = 'quickdraw.png' ?>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "We will train the model on GPU for free on Google Colab using Keras then run it on the browser directly using TensorFlow.js(tfjs) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Flatten, Convolution2D, MaxPooling2D, Dense\n",
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "Since we have a limited memory we will not train on all the classes. We will only use 100 classes of the dataset. The data for each class is available on Google Cloud as numpy arrays of the shape [N,784] where N is the number of of the images for that particular class. We first download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data_c/classes.txt') as f:\n",
    "#     classes = f.readlines()\n",
    "# classes = [c.replace('\\n', '') for c in classes]\n",
    "# classes = [c.replace(' ', '_') for c in classes]\n",
    "# print(classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['drums', 'spider', 'flowers', 'clock', 'airplane', 'face', 'sun', 'bicycle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "def download():\n",
    "    base = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
    "    list = []\n",
    "    for c in class_names:\n",
    "        cls_url = c.replace('_', '%20')\n",
    "        path = base + cls_url + '.npy'\n",
    "        list.append(path)\n",
    "        #urllib.request.urlretrieve(path, 'data/'+ c)\n",
    "    return list\n",
    "\n",
    "lista = download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('linki.txt', 'w') as f:\n",
    "    for link in lista:\n",
    "        f.write('{}\\n'.format(link))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since our memory is limited we will only load to memory 5000 images per classes.  \n",
    "# We also reserve 20% of the data unseen for testing\n",
    "\n",
    "def load_data(root, vfold_ratio = 0.2, max_items_per_class = 10000):\n",
    "    all_files = glob.glob(os.path.join(root, '*.npy'))\n",
    "    \n",
    "    x = np.empty([0, 784])\n",
    "    y = np.empty([0])\n",
    "    class_names = []\n",
    "    class_index = {}\n",
    "    \n",
    "    for file_number, file in enumerate(all_files):\n",
    "        data = np.load(file)\n",
    "        data = data[0: max_items_per_class, : ]\n",
    "        labels = np.full(data.shape[0], file_number)\n",
    "        \n",
    "        x = np.concatenate((x, data), axis = 0)\n",
    "        y = np.append(y, labels)\n",
    "        \n",
    "        class_name, ext = os.path.splitext(os.path.basename(file))\n",
    "        \n",
    "        class_names.append(class_name)\n",
    "        class_index[file_number] = class_name\n",
    "        \n",
    "    data = None\n",
    "    labels = None\n",
    "    \n",
    "    #separate into training and testing\n",
    "    permutation = np.random.permutation(y.shape[0])\n",
    "    x = x[permutation, : ]\n",
    "    y = y[permutation]\n",
    "    \n",
    "    vfold_size = int(x.shape[0]/100*(vfold_ratio * 100))\n",
    "    \n",
    "    x_test = x[0:vfold_size, :]\n",
    "    y_test = y[0:vfold_size]\n",
    "    \n",
    "    x_train = x[vfold_size: x.shape[0], :]\n",
    "    y_train = y[vfold_size:y.shape[0]]\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test, class_names, class_index\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Data\n",
    "We preprocess the data to prepare it for training.  \n",
    "The model will take batches of the shape [N, 28, 28, 1] and outputs probabilities of the shape [N, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test, class_names, class_index = load_data('data')\n",
    "\n",
    "num_classes = len(class_names)\n",
    "image_size = 28\n",
    "\n",
    "x_train = 255.0 - x_train\n",
    "x_test = 255.0 - x_test\n",
    "\n",
    "x_train /= 255.0\n",
    "x_test /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32000, 784), (32000,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'airplane',\n",
       " 1: 'bicycle',\n",
       " 2: 'clock',\n",
       " 3: 'drums',\n",
       " 4: 'face',\n",
       " 5: 'flower',\n",
       " 6: 'spider',\n",
       " 7: 'sun'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bicycle\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "obrazek = 562\n",
    "\n",
    "plt.imshow(x_train[obrazek].reshape(28,28), cmap = 'gray')\n",
    "print(class_index[y_train[obrazek]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], image_size, image_size, 1).astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0], image_size, image_size, 1).astype('float32')\n",
    "\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a simple CNN. Notice that the simpler the model with lesser number of parameters the better. Indeed, we will run the model after conversion on the browser and we want the model to run fast for prediction. The following model contains 3 conv layers and 2 dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(16, (3,3), padding='same', activation = 'relu', input_shape = x_train.shape[1:]))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(32, (3, 2), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(64, (3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "model.add(Dense(8, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['top_k_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 32)        3104      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               73856     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 1032      \n",
      "=================================================================\n",
      "Total params: 96,648\n",
      "Trainable params: 96,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28800 samples, validate on 3200 samples\n",
      "Epoch 1/10\n",
      "28800/28800 [==============================] - 30s 1ms/step - loss: 0.2856 - top_k_categorical_accuracy: 0.9941 - val_loss: 0.2900 - val_top_k_categorical_accuracy: 0.9928\n",
      "Epoch 2/10\n",
      "28800/28800 [==============================] - 30s 1ms/step - loss: 0.2571 - top_k_categorical_accuracy: 0.9950 - val_loss: 0.2768 - val_top_k_categorical_accuracy: 0.9919\n",
      "Epoch 3/10\n",
      "28800/28800 [==============================] - 30s 1ms/step - loss: 0.2369 - top_k_categorical_accuracy: 0.9951 - val_loss: 0.2655 - val_top_k_categorical_accuracy: 0.9928\n",
      "Epoch 4/10\n",
      "28800/28800 [==============================] - 31s 1ms/step - loss: 0.2187 - top_k_categorical_accuracy: 0.9955 - val_loss: 0.2571 - val_top_k_categorical_accuracy: 0.9922\n",
      "Epoch 5/10\n",
      "28800/28800 [==============================] - 36s 1ms/step - loss: 0.2006 - top_k_categorical_accuracy: 0.9959 - val_loss: 0.2419 - val_top_k_categorical_accuracy: 0.9925\n",
      "Epoch 6/10\n",
      "28800/28800 [==============================] - 31s 1ms/step - loss: 0.1931 - top_k_categorical_accuracy: 0.9965 - val_loss: 0.2527 - val_top_k_categorical_accuracy: 0.9928\n",
      "Epoch 7/10\n",
      "28800/28800 [==============================] - 33s 1ms/step - loss: 0.1764 - top_k_categorical_accuracy: 0.9971 - val_loss: 0.2329 - val_top_k_categorical_accuracy: 0.9922\n",
      "Epoch 8/10\n",
      "28800/28800 [==============================] - 36s 1ms/step - loss: 0.1658 - top_k_categorical_accuracy: 0.9971 - val_loss: 0.2396 - val_top_k_categorical_accuracy: 0.9909\n",
      "Epoch 9/10\n",
      "28800/28800 [==============================] - 34s 1ms/step - loss: 0.1577 - top_k_categorical_accuracy: 0.9976 - val_loss: 0.2373 - val_top_k_categorical_accuracy: 0.9925\n",
      "Epoch 10/10\n",
      "11520/28800 [===========>..................] - ETA: 19s - loss: 0.1553 - top_k_categorical_accuracy: 0.9980"
     ]
    }
   ],
   "source": [
    "model.fit(x=x_train, y=y_train, validation_split=0.1, batch_size=256, epochs=10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 3s 338us/step\n",
      "0.90\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test)\n",
    "print(f'{score[1]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the model for Web Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('quickdraw.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflowjs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!tensorflowjs_converter --input_format keras quickdraw.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
